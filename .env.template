###############################################################################
#               agent‑generator  —  example environment file                  #
#                                                                             #
#  1. Duplicate this file as `.env` in the repository root (same directory    #
#     as pyproject.toml).                                                     #
#  2. Fill in the placeholders with *your* credentials / preferred defaults.  #
#  3. `source .env`  (or let `direnv` / Docker compose handle it).            #
###############################################################################

# ────────────────────────────────────────────────────────────────────────────
#  IBM WatsonX  (default provider)
# ────────────────────────────────────────────────────────────────────────────
WATSONX_API_KEY="paste-your-watsonx-api-key"
WATSONX_PROJECT_ID="paste-your-project-id"
# Change region if needed, e.g.  eu-gb.ml.cloud.ibm.com
WATSONX_URL="https://us-south.ml.cloud.ibm.com"

# ────────────────────────────────────────────────────────────────────────────
#  Optional: OpenAI
# ────────────────────────────────────────────────────────────────────────────
OPENAI_API_KEY="paste-your-openai-api-key"

# ────────────────────────────────────────────────────────────────────────────
#  agent‑generator overrides (all AGENTGEN_ prefix)
# ────────────────────────────────────────────────────────────────────────────
AGENTGEN_PROVIDER="watsonx"                    # watsonx | openai
AGENTGEN_MODEL="meta-llama-3-70b-instruct"     # default model (override via CLI)
AGENTGEN_TEMPERATURE="0.7"
AGENTGEN_MAX_TOKENS="4096"
AGENTGEN_LOG_LEVEL="INFO"                      # DEBUG | INFO | WARNING | ERROR

# Default port for FastAPI MCP wrappers (Python frameworks only)
AGENTGEN_MCP_DEFAULT_PORT="8080"
