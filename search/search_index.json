{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udee0\ufe0f agent\u2011generator","text":"<p>A one\u2011line prompt \u2026 a fully configured multi\u2011agent team, served via an MCP Gateway.</p> <p></p> <p><code>agent\u2011generator</code> converts plain\u2011English requirements into runnable code or YAML workflows for:</p> <ul> <li>CrewAI &amp; CrewAI\u00a0Flow </li> <li>LangGraph </li> <li>ReAct </li> <li>WatsonX\u00a0Orchestrate (YAML)</li> </ul> <p>You can choose your LLM provider:</p> <ul> <li>IBM\u00a0WatsonX (default)  </li> <li>OpenAI (via <code>--provider openai</code> or <code>AGENTGEN_PROVIDER=openai</code>)</li> </ul>"},{"location":"#why-multiagent-mcp-gateway","title":"Why Multi\u2011Agent + MCP Gateway?","text":"<p>Splitting a complex task into specialized agents lets you:</p> <ul> <li>Parallelize subtasks (e.g. research, summarise, review)  </li> <li>Specialize prompts per role for cleaner, maintainable code  </li> <li>Visualize your workflow as Mermaid/DOT diagrams  </li> </ul> <p>The MCP server wrapper (FastAPI) and MCP Gateway provide:</p> <ul> <li>Unified HTTP entrypoint for all agents  </li> <li>Route isolation per task or agent  </li> <li>Built\u2011in monitoring, health checks, and cost endpoints  </li> </ul> <p>This makes your multi\u2011agent system production\u2011ready with minimal boilerplate.</p>"},{"location":"#highlights","title":"Highlights","text":"Feature \u2713 IBM\u00a0WatsonX default \u2705 OpenAI support \u2705 Plug\u2011in providers \u2705 MCP server wrapper &amp; Gateway \u2705 Rich CLI + Flask UI \u2705 Mermaid / DOT diagrams \u2705 Quick install <pre><code>pip install agent-generator[web,openai]\n</code></pre> <p>Jump in: Installation \u279c \u00b7 Usage \u279c \u00b7 Frameworks \u279c</p>"},{"location":"architecture/","title":"Architecture of agent-generator","text":"<p>This document describes the overall architecture of agent-generator, with a focus on the generative pipeline that transforms plain\u2011English prompts into fully configured agent code or YAML for various frameworks.</p>"},{"location":"architecture/#1-highlevel-overview","title":"1. High\u2011Level Overview","text":"<ol> <li>User Input (CLI or Web UI): The user provides a natural\u2011language requirement, selects a framework, and optionally a provider, model, and other flags.</li> <li>Env Loader: <code>.env</code> variables are loaded into the process environment.</li> <li>Pre\u2011flight Check: Credentials for the chosen provider (WatsonX or OpenAI) are validated; helpful messages are printed if missing.</li> <li>Settings Loader: Pydantic reads <code>AGENTGEN_</code> environment variables and <code>.env</code> to build a <code>Settings</code> object, applying defaults and provider\u2011specific overrides (e.g., defaulting <code>gpt-4o</code> for OpenAI).</li> <li>Parser: The natural\u2011language prompt is parsed into a Workflow graph: a set of <code>Agent</code>, <code>Task</code>, and <code>Edge</code> objects that represent the multi\u2011agent workflow.</li> <li>Prompt Renderer: A Jinja template combines the workflow, settings, and framework instructions into a single string prompt for the LLM.</li> <li>LLM Provider: The prompt is sent to the selected LLM provider (WatsonX REST API or OpenAI SDK). The raw completion is returned.</li> <li>Framework Generator: Another set of Jinja templates consume the <code>Workflow</code> and generate code or YAML in the target framework\u2019s syntax, leveraging the LLM response only when needed (e.g., to fill details in templates).</li> <li>Output: The generated code/YAML is either printed with syntax highlighting or written to a file. Token\u2011 and cost\u2011estimates are optionally shown.</li> </ol>"},{"location":"architecture/#2-component-breakdown","title":"2. Component Breakdown","text":""},{"location":"architecture/#21-cli-web-ui","title":"2.1 CLI / Web UI","text":"<ul> <li><code>src/agent_generator/cli.py</code>: Typer\u2011based CLI entrypoint for terminal usage.</li> <li><code>src/agent_generator/web/</code>: Flask application providing a browser UI with forms for prompt, framework, and provider selection.</li> </ul>"},{"location":"architecture/#22-configuration-environment","title":"2.2 Configuration &amp; Environment","text":"<ul> <li><code>config.py</code>: Defines <code>Settings</code> model reading from <code>AGENTGEN_*</code> vars. Handles provider defaults, model overrides, and credential checks.</li> <li><code>.env</code> support: Load dotenv at startup to override environment values locally.</li> </ul>"},{"location":"architecture/#23-parser-workflow-model","title":"2.3 Parser &amp; Workflow Model","text":"<ul> <li> <p><code>utils/parser.py</code>: Converts free\u2011form English into a structured <code>Workflow</code>:</p> </li> <li> <p><code>Agent</code> model: <code>id</code>, <code>role</code>, <code>tools</code>, <code>llm_config</code>.</p> </li> <li><code>Task</code> model: <code>id</code>, <code>goal</code>, <code>inputs</code>, <code>outputs</code>, <code>agent_id</code>.</li> <li><code>Edge</code> model (future extension): dependencies between tasks.</li> </ul>"},{"location":"architecture/#24-prompt-rendering","title":"2.4 Prompt Rendering","text":"<ul> <li><code>utils/prompts.py</code>: Jinja templates to render the full LLM prompt, injecting workflow and settings. Although currently not always used for code generation, it standardizes prompt creation.</li> </ul>"},{"location":"architecture/#25-llm-providers","title":"2.5 LLM Providers","text":"<ul> <li><code>providers/watsonx_provider.py</code>: Thin wrapper around IBM WatsonX REST API.</li> <li><code>providers/openai_provider.py</code>: Wraps the OpenAI Python SDK; defaults to <code>gpt-4o</code> if no override.</li> <li><code>providers/base.py</code>: Shared interface (<code>generate</code>, <code>tokenize</code>, <code>estimate_cost</code>).</li> </ul>"},{"location":"architecture/#26-framework-generators","title":"2.6 Framework Generators","text":"<p>Each framework lives in <code>src/agent_generator/frameworks/&lt;name&gt;/generator.py</code> and implements:</p> <ul> <li><code>file_extension</code>: <code>py</code> or <code>yaml</code>.</li> <li><code>generate_code(workflow, settings, mcp)</code>: Renders Jinja templates (<code>agent.jinja2</code>, <code>task.jinja2</code>, <code>main.jinja2</code>).</li> </ul> <p>Supported frameworks:</p> <ul> <li>CrewAI (Python SDK)</li> <li>CrewAI Flow (event\u2011driven)</li> <li>LangGraph (DAG API)</li> <li>ReAct (reason\u2011act pattern)</li> <li>WatsonX Orchestrate (native YAML)</li> </ul>"},{"location":"architecture/#27-output-utilities","title":"2.7 Output &amp; Utilities","text":"<ul> <li>Syntax highlighting via Rich when printing.</li> <li>MCP wrapper: Optional FastAPI server scaffold if <code>--mcp</code> is passed.</li> </ul>"},{"location":"architecture/#3-generative-pipeline-detail","title":"3. Generative Pipeline Detail","text":""},{"location":"architecture/#31-detailed-data-flow","title":"3.1 Detailed Data Flow","text":"<pre><code>flowchart TD\n  P[User Prompt] --&gt; Parse\n  Parse[Parser \u2192 Workflow] --&gt; PromptRender\n  PromptRender[Template Prompt] --&gt; LLM\n  LLM[LLM Provider] --&gt; Completion\n  Completion --&gt; Generator[Framework Generator]\n  Generator --&gt; Code[Generated Code/YAML]\n</code></pre> <ol> <li>Parser: Breaks text into structured data.</li> <li>PromptRender: Applies Jinja to produce the text sent to the LLM.</li> <li>LLM: Returns a completion string with e.g. helper instructions or detailed sub\u2011prompts.</li> <li>Generator: Ignores most LLM output, instead using static templates to turn the <code>Workflow</code> graph into code.</li> <li>Code: Final output, ready to run or deploy.</li> </ol>"},{"location":"architecture/#why-separate-prompt-vs-generator","title":"Why separate prompt vs generator?","text":"<ul> <li>Decouples what you ask the LLM (open\u2011ended natural\u2011language) from how you produce deterministic code templates.</li> <li>Future architectures can inject LLM responses deeper into code gen templates.</li> </ul>"},{"location":"architecture/#4-extensibility-customization","title":"4. Extensibility &amp; Customization","text":"<ul> <li>Adding a Provider: Create <code>providers/my_provider.py</code> implementing <code>BaseProvider</code>, register in <code>PROVIDERS</code> in <code>providers/__init__.py</code>.</li> <li>Adding a Framework: Create <code>frameworks/my_framework/generator.py</code>, add templates under that folder, register in <code>FRAMEWORKS</code> in <code>frameworks/__init__.py</code>.</li> <li>Custom Prompts: Modify or add Jinja templates in <code>utils/prompts.py</code>.</li> <li>Parser tweaks: Improve <code>utils/parser.py</code> to extract richer workflows (e.g. data schemas, error handling agents).</li> </ul>"},{"location":"architecture/#5-file-locations","title":"5. File Locations","text":"<pre><code>src/agent_generator/\n\u251c\u2500\u2500 cli.py\n\u251c\u2500\u2500 config.py\n\u251c\u2500\u2500 frameworks/\n\u2502   \u2514\u2500\u2500 &lt;framework&gt;/generator.py\n\u251c\u2500\u2500 providers/\n\u2502   \u2514\u2500\u2500 *_provider.py\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 parser.py\n    \u2514\u2500\u2500 prompts.py\n</code></pre> <p>Jump in: Installation \u279c \u00b7 Usage \u279c \u00b7 Frameworks \u279c</p>"},{"location":"frameworks/","title":"Usage Guide","text":"<p>This page covers common workflows for both the CLI and the Flask\u202fWeb\u00a0UI. For installation instructions see Installation.</p>"},{"location":"frameworks/#1-commandline-interface","title":"1\u00a0\u00a0Command\u2011line interface","text":""},{"location":"frameworks/#11-basic-syntax","title":"1.1\u00a0Basic syntax","text":"<pre><code>agent-generator [OPTIONS] \"plain\u2011English requirement\"\n</code></pre>"},{"location":"frameworks/#12-frequently-used-flags","title":"1.2\u00a0Frequently used flags","text":"Flag / Option Description Example <code>-f, --framework</code>\u202f* Which generator to use (<code>crewai</code>, <code>langgraph</code>, \u2026). <code>--framework crewai</code> <code>-p, --provider</code> LLM back\u2011end (<code>watsonx</code> default, or <code>openai</code>). <code>--provider openai</code> <code>--model</code> Override default model for the provider. <code>--model gpt-4o</code> <code>--temperature</code> Sampling randomness (0\u20132). <code>--temperature 0.3</code> <code>--max-tokens</code> Response length cap. <code>--max-tokens 2048</code> <code>--mcp / --no-mcp</code> Wrap Python output in an MCP FastAPI server. <code>--mcp</code> <code>-o, --output PATH</code> Write result to file instead of stdout. <code>-o team.py</code> <code>--dry-run</code> Build workflow + code skeleton but skip LLM call. <code>--dry-run</code> <code>--show-cost</code> Print token counts &amp; approximate USD cost. <code>--show-cost</code>"},{"location":"frameworks/#13-common-recipes","title":"1.3\u00a0Common recipes","text":"Goal Command Orchestrate YAML from one\u2011liner <code>agent-generator \"Email summariser\" -f watsonx_orchestrate -o summariser.yaml</code> CrewAI Flow with MCP wrapper <code>agent-generator \"Analyse tweets\" -f crewai_flow --mcp -o tweets_flow.py</code> Cost estimate only <code>agent-generator \"Scrape website\" -f react --dry-run --show-cost</code> Use OpenAI instead of WatsonX <code>agent-generator \"Write jokes\" -f react -p openai --model gpt-4o</code>"},{"location":"frameworks/#2-flask-web-ui","title":"2\u00a0\u00a0Flask Web\u00a0UI","text":""},{"location":"frameworks/#21-run-locally","title":"2.1\u00a0Run locally","text":"<pre><code>FLASK_APP=agent_generator.web FLASK_ENV=development flask run\n# visit http://localhost:5000\n</code></pre>"},{"location":"frameworks/#22-workflow","title":"2.2\u00a0Workflow","text":"<ol> <li>Fill in prompt \u2013 describe your requirement.</li> <li>Pick framework &amp; provider \u2013 drop\u2011downs.</li> <li>(Optional) toggle MCP wrapper.</li> <li>Click Generate.</li> <li>Download the code/YAML or copy\u2011paste from the preview.</li> <li>Mermaid diagram appears under the code for quick validation.</li> </ol>"},{"location":"frameworks/#3-docker-usage","title":"3\u00a0\u00a0Docker usage","text":"<pre><code>docker build -t agent-generator .\ndocker run -e WATSONX_API_KEY=... -e WATSONX_PROJECT_ID=... \\\n           -p 8000:8000 agent-generator\n# Web UI \u2192 http://localhost:8000\n</code></pre> <p>You can also exec into the container to run the CLI:</p> <pre><code>docker run --rm agent-generator agent-generator \"Say hi\" -f react --dry-run\n</code></pre>"},{"location":"frameworks/#4-serving-generated-mcp-skills","title":"4\u00a0\u00a0Serving generated MCP skills","text":"<p>Every Python framework (<code>crewai</code>, <code>crewai_flow</code>, <code>langgraph</code>, <code>react</code>) can be generated with an MCP wrapper:</p> <pre><code>agent-generator \"...data pipeline...\" -f langgraph --mcp -o pipeline.py\npython pipeline.py serve      # exposes POST /invoke on port\u00a08080\n</code></pre> <p>Upload the packaged script or its Docker image to your MCP\u00a0Gateway and then import it as a custom skill in WatsonX\u00a0Orchestrate.</p>"},{"location":"frameworks/#5-troubleshooting","title":"5\u00a0\u00a0Troubleshooting","text":"Symptom Resolution CLI raises\u00a0401 (WatsonX) Verify <code>WATSONX_API_KEY</code>, <code>WATSONX_PROJECT_ID</code>, region URL. <code>ModuleNotFoundError: flask</code> <code>pip install \"agent-generator[web]\"</code> Diagram doesn\u2019t render in UI Check browser console \u2013 Mermaid JS must load (make sure <code>unpkg.com</code> isn\u2019t blocked). High cost estimate Lower <code>--max-tokens</code> or pick <code>llama\u20113\u20118b</code> instead. Gateway import fails Ensure you used <code>--mcp</code> and port\u00a08080 is exposed. <p>Jump in: Installation \u279c \u00b7 Usage \u279c \u00b7 Frameworks \u279c</p>"},{"location":"installation/","title":"Installation Guide","text":"<p>agent-generator supports Python \u2265 3.9 and ships with IBM WatsonX as the default LLM provider. Optional extras unlock OpenAI support, the Flask Web UI, and developer tooling.</p>"},{"location":"installation/#windows-via-wsl","title":"Windows (via WSL)","text":"<p>If you\u2019re on Windows, we recommend using WSL (Windows Subsystem for Linux) to get a Linux-like environment.</p> <p>Enable WSL     Open PowerShell as Administrator and run:</p> <pre><code>```powershell\nwsl --install\n```\n\nWhen it finishes, reboot your PC and launch your new Linux distro (e.g., Ubuntu).\n</code></pre> <p>Create &amp; activate a virtual environment     In your WSL terminal:</p> <pre><code>```bash\npython3 -m venv venv\nsource venv/bin/activate\n```\n</code></pre> <p>Upgrade pip &amp; install agent-generator</p> <pre><code>```bash\npip install --upgrade pip --break-system-packages\npip install \"agent-generator[dev,web,openai]\" --break-system-packages\n```\n\nIf you encounter PEP 668 \u201cexternally managed environment\u201d errors inside the `venv`, continue using the `--break-system-packages` flag.\n</code></pre> <p>Prepare the <code>.env</code> file</p> <pre><code>At the project root (next to `Makefile`, `pyproject.toml`, etc.), create a file named `.env`:\n</code></pre> <pre><code>WATSONX_API_KEY=your_watsonx_key\nWATSONX_PROJECT_ID=your_watsonx_project_id\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\n# (Optional) Uncomment for OpenAI\n# OPENAI_API_KEY=sk-...\n</code></pre> <p>Load environment variables into your shell</p> <p>Still inside WSL, choose one method:</p> <pre><code># Method A: Source all at once\nset -a &amp;&amp; source .env &amp;&amp; set +a\n# Method B: Strip Windows carriage returns and export\nexport $(cat .env | tr -d '\\r' | xargs)\n</code></pre> <pre><code>Verify that the variables were loaded correctly:\n</code></pre> <pre><code>echo \"$WATSONX_API_KEY\"\necho \"$WATSONX_PROJECT_ID\"\n</code></pre> <p>Run the generator     To generate an agent using the default WatsonX provider:</p> <pre><code>```bash\nagent-generator \\\n  \"I need a research assistant that summarises papers\" \\\n  --framework watsonx_orchestrate \\\n  --output research_assistant.yaml\n```\n\nOr, to use the OpenAI provider:\n\n```bash\nexport OPENAI_API_KEY=sk-...\nagent-generator \\\n  \"I need a research assistant that summarises papers\" \\\n  --framework crewai \\\n  --provider openai \\\n  --output research_assistant.py\n```\n</code></pre>"},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<p>To install the core package with WatsonX support only, run:</p> <pre><code>pip install agent-generator\n</code></pre> <p>This gives you:</p> <ul> <li>The CLI (<code>agent-generator \u2026</code>)</li> <li>Core runtime dependencies</li> <li>The WatsonX provider (default model: <code>meta-llama/llama-3-3-70b-instruct</code>)</li> </ul>"},{"location":"installation/#optional-extras","title":"Optional Extras","text":"<p>You can install extra dependencies for more features.</p> Extra tag Installs\u2026 When to use it <code>openai</code> <code>openai</code>, <code>tiktoken</code> Generate code with GPT models <code>web</code> <code>flask</code>, <code>gunicorn</code> Run the visual Web UI <code>dev</code> <code>pytest</code>, <code>ruff</code>, <code>mypy</code>, <code>mkdocs</code>, <code>pre-commit</code> Contributing or running tests <p>Examples:</p> <pre><code># Core + Web UI\npip install \"agent-generator[web]\"\n\n# Core + OpenAI\npip install \"agent-generator[openai]\"\n\n# Install everything\npip install \"agent-generator[dev,web,openai]\"\n</code></pre>"},{"location":"installation/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project's root directory or export these variables in your shell.</p> <pre><code># --- PROVIDER SELECTION ---\n# Choose one: \"watsonx\" (default) or \"openai\"\nAGENTGEN_PROVIDER=watsonx\n\n# --- IBM WatsonX Configuration (if provider is \"watsonx\") ---\nWATSONX_API_KEY=\"your_watsonx_api_key_here\"\nWATSONX_PROJECT_ID=\"your_watsonx_project_id_here\"\nWATSONX_URL=\"https://us-south.ml.cloud.ibm.com\"\n\n# --- OpenAI Configuration (if provider is \"openai\") ---\n# OPENAI_API_KEY=\"sk-your_openai_api_key_here\"\n\n# --- OPTIONAL OVERRIDES ---\n# AGENTGEN_MODEL=\"meta-llama/llama-3-70b-instruct\"\n# AGENTGEN_TEMPERATURE=\"0.7\"\n# AGENTGEN_MAX_TOKENS=\"4096\"\n</code></pre> <p>Tip: Add <code>.env</code> to your IDE\u2019s environment or use a tool like direnv for automatic loading.</p>"},{"location":"installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>Run a dry run to check if the CLI is working correctly.</p> <pre><code>agent-generator \"Say hello\" --framework react --dry-run --show-cost\n</code></pre> <p>You should see output similar to this:</p> <pre><code>\u2248 prompt_tokens=7, completion_tokens=42, est. cost=$0.0001\n# Auto-generated ReAct agent\nimport json\n...\n</code></pre>"},{"location":"installation/#running-the-web-ui","title":"Running the Web UI","text":"<p>You can run the web interface using either the Flask development server or Docker.</p> <p>Development Server (with hot-reload):</p> <pre><code>FLASK_APP=agent_generator.web FLASK_ENV=development flask run\n</code></pre> <p>Access the UI at <code>http://localhost:5000</code>.</p> <p>Production (via Docker):</p> <pre><code>docker build -t agentgen .\ndocker run -e WATSONX_API_KEY=... -p 8000:8000 agentgen\n</code></pre> <p>Access the UI at <code>http://localhost:8000</code>.</p>"},{"location":"installation/#upgrading","title":"Upgrading","text":"<p>To upgrade the package to the latest version:</p> <pre><code>pip install --upgrade agent-generator\n</code></pre> <p>If you installed any optional extras, you must re-specify them during the upgrade:</p> <p><code>bash pip install --upgrade \"agent-generator[web,openai]\"</code></p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"Issue Solution <code>401 Unauthorized (WatsonX)</code> Check that <code>WATSONX_API_KEY</code> and <code>WATSONX_PROJECT_ID</code> are correct and exported. <code>ModuleNotFoundError: flask</code> You need the <code>web</code> extra. Run <code>pip install \"agent-generator[web]\"</code>. CLI hangs or times out Lower the value of <code>--max-tokens</code>, check your network connection, or try a different provider like <code>--provider openai</code>. Mermaid diagram not rendering (UI) Ensure your browser has internet access to the CDN at <code>unpkg.com</code>. <p>Jump in: Installation \u279c \u00b7 Usage \u279c \u00b7 Frameworks \u279c</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This page covers common workflows for both the CLI and the Flask\u202fWeb\u00a0UI. For installation instructions see Installation.</p>"},{"location":"usage/#1-commandline-interface","title":"1\u00a0\u00a0Command\u2011line interface","text":""},{"location":"usage/#11-basic-syntax","title":"1.1\u00a0Basic syntax","text":"<pre><code>agent-generator [OPTIONS] \"plain\u2011English requirement\"\n</code></pre>"},{"location":"usage/#12-frequently-used-flags","title":"1.2\u00a0Frequently used flags","text":"Flag / Option Description Example <code>-f, --framework</code>\u202f* Which generator to use (<code>crewai</code>, <code>langgraph</code>, \u2026). <code>--framework crewai</code> <code>-p, --provider</code> LLM back\u2011end (<code>watsonx</code> default, or <code>openai</code>). <code>--provider openai</code> <code>--model</code> Override default model for the provider. <code>--model gpt-4o</code> <code>--temperature</code> Sampling randomness (0\u20132). <code>--temperature 0.3</code> <code>--max-tokens</code> Response length cap. <code>--max-tokens 2048</code> <code>--mcp / --no-mcp</code> Wrap Python output in an MCP FastAPI server. <code>--mcp</code> <code>-o, --output PATH</code> Write result to file instead of stdout. <code>-o team.py</code> <code>--dry-run</code> Build workflow + code skeleton but skip LLM call. <code>--dry-run</code> <code>--show-cost</code> Print token counts &amp; approximate USD cost. <code>--show-cost</code>"},{"location":"usage/#13-common-recipes","title":"1.3\u00a0Common recipes","text":"Goal Command Orchestrate YAML from one\u2011liner <code>agent-generator \"Email summariser\" -f watsonx_orchestrate -o summariser.yaml</code> CrewAI Flow with MCP wrapper <code>agent-generator \"Analyse tweets\" -f crewai_flow --mcp -o tweets_flow.py</code> Cost estimate only <code>agent-generator \"Scrape website\" -f react --dry-run --show-cost</code> Use OpenAI instead of WatsonX <code>agent-generator \"Write jokes\" -f react -p openai --model gpt-4o</code>"},{"location":"usage/#2-flask-web-ui","title":"2\u00a0\u00a0Flask Web\u00a0UI","text":""},{"location":"usage/#21-run-locally","title":"2.1\u00a0Run locally","text":"<pre><code>FLASK_APP=agent_generator.web FLASK_ENV=development flask run\n# visit http://localhost:5000\n</code></pre>"},{"location":"usage/#22-workflow","title":"2.2\u00a0Workflow","text":"<ol> <li>Fill in prompt \u2013 describe your requirement.</li> <li>Pick framework &amp; provider \u2013 drop\u2011downs.</li> <li>(Optional) toggle MCP wrapper.</li> <li>Click Generate.</li> <li>Download the code/YAML or copy\u2011paste from the preview.</li> <li>Mermaid diagram appears under the code for quick validation.</li> </ol>"},{"location":"usage/#3-docker-usage","title":"3\u00a0\u00a0Docker usage","text":"<pre><code>docker build -t agent-generator .\ndocker run -e WATSONX_API_KEY=... -e WATSONX_PROJECT_ID=... \\\n           -p 8000:8000 agent-generator\n# Web UI \u2192 http://localhost:8000\n</code></pre> <p>You can also exec into the container to run the CLI:</p> <pre><code>docker run --rm agent-generator agent-generator \"Say hi\" -f react --dry-run\n</code></pre>"},{"location":"usage/#4-serving-generated-mcp-skills","title":"4\u00a0\u00a0Serving generated MCP skills","text":"<p>Every Python framework (<code>crewai</code>, <code>crewai_flow</code>, <code>langgraph</code>, <code>react</code>) can be generated with an MCP wrapper:</p> <pre><code>agent-generator \"...data pipeline...\" -f langgraph --mcp -o pipeline.py\npython pipeline.py serve      # exposes POST /invoke on port\u00a08080\n</code></pre> <p>Upload the packaged script or its Docker image to your MCP\u00a0Gateway and then import it as a custom skill in WatsonX\u00a0Orchestrate.</p>"},{"location":"usage/#5-troubleshooting","title":"5\u00a0\u00a0Troubleshooting","text":"Symptom Resolution CLI raises\u00a0401 (WatsonX) Verify <code>WATSONX_API_KEY</code>, <code>WATSONX_PROJECT_ID</code>, region URL. <code>ModuleNotFoundError: flask</code> <code>pip install \"agent-generator[web]\"</code> Diagram doesn\u2019t render in UI Check browser console \u2013 Mermaid JS must load (make sure <code>unpkg.com</code> isn\u2019t blocked). High cost estimate Lower <code>--max-tokens</code> or pick <code>llama\u20113\u20118b</code> instead. Gateway import fails Ensure you used <code>--mcp</code> and port\u00a08080 is exposed. <p>Still stuck?\u00a0Open an issue on the GitHub tracker.</p> <p>Jump in: Installation \u279c \u00b7 Usage \u279c \u00b7 Frameworks \u279c</p>"}]}